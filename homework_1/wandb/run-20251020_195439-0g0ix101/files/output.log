Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 110376.42it/s]
Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 211.05it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
/workspace/llm/your_solution.py:273: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                        | 0/60627 [00:00<?, ?it/s]W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0] or:
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py", line 969, in wrapper
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 730, in forward
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py", line 969, in wrapper
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 446, in forward
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 503, in _update_causal_mask
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:55:33.790000 7904 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:56:00.566000 7904 torch/_dynamo/convert_frame.py:861] [9/8] torch._dynamo hit config.cache_size_limit (8)
W1020 19:56:00.566000 7904 torch/_dynamo/convert_frame.py:861] [9/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)
W1020 19:56:00.566000 7904 torch/_dynamo/convert_frame.py:861] [9/8]    last reason: 9/0: L['self'].layer_idx == 0
W1020 19:56:00.566000 7904 torch/_dynamo/convert_frame.py:861] [9/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1020 19:56:00.566000 7904 torch/_dynamo/convert_frame.py:861] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
  1%|â–Š                                                                                                                                          | 366/60627 [08:13<10:09:44,  1.65it/s]Traceback (most recent call last):
{'loss': 8.7548, 'grad_norm': 1.71875, 'learning_rate': 0.00019800000000000002, 'epoch': 0.0}
  File "/workspace/llm/your_solution.py", line 296, in <module>                                                                                                                        
{'eval_loss': 7.697390556335449, 'eval_runtime': 44.7291, 'eval_samples_per_second': 111.784, 'eval_steps_per_second': 13.973, 'epoch': 0.0}
{'loss': 6.7489, 'grad_norm': 0.8984375, 'learning_rate': 0.000398, 'epoch': 0.0}
{'eval_loss': 6.654306411743164, 'eval_runtime': 43.5723, 'eval_samples_per_second': 114.752, 'eval_steps_per_second': 14.344, 'epoch': 0.0}
{'loss': 5.7101, 'grad_norm': 0.8359375, 'learning_rate': 0.0003993446638092244, 'epoch': 0.0}
{'eval_loss': 6.023236274719238, 'eval_runtime': 43.6316, 'eval_samples_per_second': 114.596, 'eval_steps_per_second': 14.324, 'epoch': 0.0}
    train_model()
  File "/workspace/llm/your_solution.py", line 281, in train_model
    trainer.train()
  File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 3791, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py", line 2454, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7fcfff8b14e0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/usr/local/lib/python3.12/dist-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
