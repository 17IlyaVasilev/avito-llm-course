Resolving data files: 100%|█████████████████████████████████████████████| 32/32 [00:00<00:00, 107374.18it/s]
Loading dataset shards: 100%|█████████████████████████████████████████████| 46/46 [00:00<00:00, 9014.11it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
/workspace/llm/your_solution.py:290: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                             | 0/60627 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] Graph break from `Tensor.item()`, consider setting:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     torch._dynamo.config.capture_scalar_outputs = True
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] or:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] to include these operations in the captured graph.
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] Graph break: from user code at:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 446, in torch_dynamo_resume_in_forward_at_422
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     causal_mask = self._update_causal_mask(
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 510, in _update_causal_mask
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     if attention_mask is not None and 0.0 in attention_mask:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
  2%|█                                                              | 1001/60627 [31:09<30:55:56,  1.87s/it]
{'loss': 8.954, 'grad_norm': 2.296875, 'learning_rate': 0.00014849999999999998, 'epoch': 0.0}
                                                                                                            
{'eval_loss': 7.737006187438965, 'eval_runtime': 44.3063, 'eval_samples_per_second': 112.851, 'eval_steps_per_second': 14.106, 'epoch': 0.0}
{'loss': 6.932, 'grad_norm': 1.125, 'learning_rate': 0.0002985, 'epoch': 0.0}
{'eval_loss': 6.775595188140869, 'eval_runtime': 43.6923, 'eval_samples_per_second': 114.437, 'eval_steps_per_second': 14.305, 'epoch': 0.0}
{'loss': 5.8542, 'grad_norm': 0.87109375, 'learning_rate': 0.00029999801313494154, 'epoch': 0.0}
{'eval_loss': 6.151052951812744, 'eval_runtime': 43.7458, 'eval_samples_per_second': 114.297, 'eval_steps_per_second': 14.287, 'epoch': 0.0}
{'loss': 5.3502, 'grad_norm': 0.89453125, 'learning_rate': 0.00029999197211355555, 'epoch': 0.01}
{'eval_loss': 5.767416954040527, 'eval_runtime': 63.268, 'eval_samples_per_second': 79.029, 'eval_steps_per_second': 9.879, 'epoch': 0.01}
{'loss': 5.0198, 'grad_norm': 0.7578125, 'learning_rate': 0.0002999818768964062, 'epoch': 0.01}
{'eval_loss': 5.479275226593018, 'eval_runtime': 43.8538, 'eval_samples_per_second': 114.015, 'eval_steps_per_second': 14.252, 'epoch': 0.01}
{'loss': 4.7875, 'grad_norm': 0.703125, 'learning_rate': 0.00029996772775636136, 'epoch': 0.01}
{'eval_loss': 5.29154109954834, 'eval_runtime': 43.8241, 'eval_samples_per_second': 114.093, 'eval_steps_per_second': 14.262, 'epoch': 0.01}
{'loss': 4.6109, 'grad_norm': 0.75, 'learning_rate': 0.0002999495250758639, 'epoch': 0.01}
{'eval_loss': 5.144778251647949, 'eval_runtime': 55.7731, 'eval_samples_per_second': 89.649, 'eval_steps_per_second': 11.206, 'epoch': 0.01}
{'loss': 4.4537, 'grad_norm': 0.72265625, 'learning_rate': 0.00029992726934692184, 'epoch': 0.01}
{'eval_loss': 5.006770133972168, 'eval_runtime': 43.8403, 'eval_samples_per_second': 114.05, 'eval_steps_per_second': 14.256, 'epoch': 0.01}
{'loss': 4.315, 'grad_norm': 0.69921875, 'learning_rate': 0.0002999009611710946, 'epoch': 0.01}
{'eval_loss': 4.89906644821167, 'eval_runtime': 43.8055, 'eval_samples_per_second': 114.141, 'eval_steps_per_second': 14.268, 'epoch': 0.01}
{'loss': 4.2174, 'grad_norm': 0.63671875, 'learning_rate': 0.0002998706012594768, 'epoch': 0.02}
{'eval_loss': 4.817725658416748, 'eval_runtime': 43.7663, 'eval_samples_per_second': 114.243, 'eval_steps_per_second': 14.28, 'epoch': 0.02}
Training stopped after 1868.65 seconds
{'train_runtime': 1869.4638, 'train_samples_per_second': 1037.764, 'train_steps_per_second': 32.43, 'train_loss': 5.447983897768415, 'epoch': 0.02}

Training complete.

=== Sample generation after training ===

В древние времена, когда люди только начинали записывать историю, чтобы быть следствием и его применения.

В 2006 году на открытии, однако, во время встречи между «Академии» в результате «Паломрания, или Джамбини и Пьюко даге, в том числе и его к пониманию, и в частности, по результатам «Паманки».

В 2017 году она выступала против «Катании» в категории

=======================================

Running final evaluation...
Final evaluation results: {'eval_loss': 4.817725658416748, 'eval_runtime': 79.4371, 'eval_samples_per_second': 62.943, 'eval_steps_per_second': 7.868, 'epoch': 0.016510795520147788}
