Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 107374.18it/s]
Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 9014.11it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
/workspace/llm/your_solution.py:290: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                             | 0/60627 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] Graph break from `Tensor.item()`, consider setting:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     torch._dynamo.config.capture_scalar_outputs = True
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] or:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] to include these operations in the captured graph.
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0] Graph break: from user code at:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 446, in torch_dynamo_resume_in_forward_at_422
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     causal_mask = self._update_causal_mask(
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 510, in _update_causal_mask
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]     if attention_mask is not None and 0.0 in attention_mask:
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
W1020 16:21:21.075000 4410 torch/_dynamo/variables/tensor.py:780] [3/0]
  2%|â–ˆ                                                              | 1001/60627 [31:09<30:55:56,  1.87s/it]
{'loss': 8.954, 'grad_norm': 2.296875, 'learning_rate': 0.00014849999999999998, 'epoch': 0.0}
                                                                                                            
{'eval_loss': 7.737006187438965, 'eval_runtime': 44.3063, 'eval_samples_per_second': 112.851, 'eval_steps_per_second': 14.106, 'epoch': 0.0}
{'loss': 6.932, 'grad_norm': 1.125, 'learning_rate': 0.0002985, 'epoch': 0.0}
{'eval_loss': 6.775595188140869, 'eval_runtime': 43.6923, 'eval_samples_per_second': 114.437, 'eval_steps_per_second': 14.305, 'epoch': 0.0}
{'loss': 5.8542, 'grad_norm': 0.87109375, 'learning_rate': 0.00029999801313494154, 'epoch': 0.0}
{'eval_loss': 6.151052951812744, 'eval_runtime': 43.7458, 'eval_samples_per_second': 114.297, 'eval_steps_per_second': 14.287, 'epoch': 0.0}
{'loss': 5.3502, 'grad_norm': 0.89453125, 'learning_rate': 0.00029999197211355555, 'epoch': 0.01}
{'eval_loss': 5.767416954040527, 'eval_runtime': 63.268, 'eval_samples_per_second': 79.029, 'eval_steps_per_second': 9.879, 'epoch': 0.01}
{'loss': 5.0198, 'grad_norm': 0.7578125, 'learning_rate': 0.0002999818768964062, 'epoch': 0.01}
{'eval_loss': 5.479275226593018, 'eval_runtime': 43.8538, 'eval_samples_per_second': 114.015, 'eval_steps_per_second': 14.252, 'epoch': 0.01}
{'loss': 4.7875, 'grad_norm': 0.703125, 'learning_rate': 0.00029996772775636136, 'epoch': 0.01}
{'eval_loss': 5.29154109954834, 'eval_runtime': 43.8241, 'eval_samples_per_second': 114.093, 'eval_steps_per_second': 14.262, 'epoch': 0.01}
{'loss': 4.6109, 'grad_norm': 0.75, 'learning_rate': 0.0002999495250758639, 'epoch': 0.01}
{'eval_loss': 5.144778251647949, 'eval_runtime': 55.7731, 'eval_samples_per_second': 89.649, 'eval_steps_per_second': 11.206, 'epoch': 0.01}
{'loss': 4.4537, 'grad_norm': 0.72265625, 'learning_rate': 0.00029992726934692184, 'epoch': 0.01}
{'eval_loss': 5.006770133972168, 'eval_runtime': 43.8403, 'eval_samples_per_second': 114.05, 'eval_steps_per_second': 14.256, 'epoch': 0.01}
{'loss': 4.315, 'grad_norm': 0.69921875, 'learning_rate': 0.0002999009611710946, 'epoch': 0.01}
{'eval_loss': 4.89906644821167, 'eval_runtime': 43.8055, 'eval_samples_per_second': 114.141, 'eval_steps_per_second': 14.268, 'epoch': 0.01}
{'loss': 4.2174, 'grad_norm': 0.63671875, 'learning_rate': 0.0002998706012594768, 'epoch': 0.02}
{'eval_loss': 4.817725658416748, 'eval_runtime': 43.7663, 'eval_samples_per_second': 114.243, 'eval_steps_per_second': 14.28, 'epoch': 0.02}
Training stopped after 1868.65 seconds
{'train_runtime': 1869.4638, 'train_samples_per_second': 1037.764, 'train_steps_per_second': 32.43, 'train_loss': 5.447983897768415, 'epoch': 0.02}

Training complete.

=== Sample generation after training ===

Ð’ Ð´Ñ€ÐµÐ²Ð½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð°, ÐºÐ¾Ð³Ð´Ð° Ð»ÑŽÐ´Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ñ‡Ð¸Ð½Ð°Ð»Ð¸ Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑÑ‚Ð²Ð¸ÐµÐ¼ Ð¸ ÐµÐ³Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ.

Ð’ 2006 Ð³Ð¾Ð´Ñƒ Ð½Ð° Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¸, Ð¾Ð´Ð½Ð°ÐºÐ¾, Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Â«ÐÐºÐ°Ð´ÐµÐ¼Ð¸Ð¸Â» Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ðµ Â«ÐŸÐ°Ð»Ð¾Ð¼Ñ€Ð°Ð½Ð¸Ñ, Ð¸Ð»Ð¸ Ð”Ð¶Ð°Ð¼Ð±Ð¸Ð½Ð¸ Ð¸ ÐŸÑŒÑŽÐºÐ¾ Ð´Ð°Ð³Ðµ, Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ð¸ ÐµÐ³Ð¾ Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ, Ð¸ Ð² Ñ‡Ð°ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸, Ð¿Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼ Â«ÐŸÐ°Ð¼Ð°Ð½ÐºÐ¸Â».

Ð’ 2017 Ð³Ð¾Ð´Ñƒ Ð¾Ð½Ð° Ð²Ñ‹ÑÑ‚ÑƒÐ¿Ð°Ð»Ð° Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² Â«ÐšÐ°Ñ‚Ð°Ð½Ð¸Ð¸Â» Ð² ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸

=======================================

Running final evaluation...
Final evaluation results: {'eval_loss': 4.817725658416748, 'eval_runtime': 79.4371, 'eval_samples_per_second': 62.943, 'eval_steps_per_second': 7.868, 'epoch': 0.016510795520147788}
