Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 108590.39it/s]
Loading dataset shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 9593.65it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1883: FutureWarning: using `--fsdp_min_num_params` is deprecated. Use fsdp_config instead
  warnings.warn("using `--fsdp_min_num_params` is deprecated. Use fsdp_config instead ", FutureWarning)
/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1892: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead
  warnings.warn(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 434, in <module>
[rank0]:     train_model()
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 407, in train_model
[rank0]:     args = TrainingArguments(
[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:   File "<string>", line 131, in __init__
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py", line 1910, in __post_init__
[rank0]:     raise ValueError("`min_num_params` and `transformer_layer_cls_to_wrap` are mutually exclusive.")
[rank0]: ValueError: `min_num_params` and `transformer_layer_cls_to_wrap` are mutually exclusive.
