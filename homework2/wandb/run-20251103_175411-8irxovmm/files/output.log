Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 99715.99it/s]
Loading dataset shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 8573.50it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
[2025-11-03 17:55:03,163] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-03 17:55:04,222] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-11-03 17:55:04,222] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/workspace/llm/avito-llm-course/homework2/your_solution.py:417: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 440, in <module>
[rank0]:     train_model()
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 425, in train_model
[rank0]:     trainer.train()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 2240, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 2322, in _inner_training_loop
[rank0]:     self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/integrations/deepspeed.py", line 444, in deepspeed_init
[rank0]:     hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/integrations/deepspeed.py", line 268, in trainer_config_finalize
[rank0]:     raise ValueError(
[rank0]: ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
[rank0]: - ds train_micro_batch_size_per_gpu=4 vs hf per_device_train_batch_size=8
[rank0]: The easiest method is to set these DeepSpeed config values to 'auto'.
