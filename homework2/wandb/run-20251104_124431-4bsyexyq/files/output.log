Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 111476.52it/s]
Loading dataset shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 7399.35it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
[2025-11-04 12:45:21,048] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-04 12:45:22,114] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-11-04 12:45:22,114] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/workspace/llm/avito-llm-course/homework2/your_solution.py:408: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 431, in <module>
[rank0]:     train_model()
[rank0]:   File "/workspace/llm/avito-llm-course/homework2/your_solution.py", line 408, in train_model
[rank0]:     trainer = Trainer(
[rank0]:               ^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 471, in __init__
[rank0]:     self.create_accelerator_and_postprocess()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py", line 5176, in create_accelerator_and_postprocess
[rank0]:     self.accelerator = Accelerator(**args)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py", line 361, in __init__
[rank0]:     deepspeed_plugins.set_mixed_precision(mixed_precision)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/accelerate/utils/dataclasses.py", line 1320, in set_mixed_precision
[rank0]:     raise ValueError(
[rank0]: ValueError: `--mixed_precision` arg cannot be set to `bf16` when `fp16` is set in the DeepSpeed config file.
