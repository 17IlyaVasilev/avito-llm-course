Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 96213.43it/s]
Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 115.99it/s]
Training samples: 1940063
Validation samples: 5000
Model pad token id: 0
Total params: 960,881,664
/workspace/llm/your_solution.py:273: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                                                                        | 0/30314 [00:00<?, ?it/s]W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0] or:
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py", line 969, in wrapper
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 730, in forward
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py", line 969, in wrapper
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 446, in forward
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]   File "/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 503, in _update_causal_mask
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:22:46.323000 6601 torch/_dynamo/variables/tensor.py:780] [0/0]
W1020 19:23:11.897000 6601 torch/_dynamo/convert_frame.py:861] [9/8] torch._dynamo hit config.cache_size_limit (8)
W1020 19:23:11.897000 6601 torch/_dynamo/convert_frame.py:861] [9/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)
W1020 19:23:11.897000 6601 torch/_dynamo/convert_frame.py:861] [9/8]    last reason: 9/0: L['self'].layer_idx == 0
W1020 19:23:11.897000 6601 torch/_dynamo/convert_frame.py:861] [9/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1020 19:23:11.897000 6601 torch/_dynamo/convert_frame.py:861] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                      | 959/30314 [30:01<15:18:57,  1.88s/it]
{'loss': 8.6555, 'grad_norm': 1.2734375, 'learning_rate': 0.00019800000000000002, 'epoch': 0.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:43<00:00, 14.46it/s]
{'eval_loss': 7.450065612792969, 'eval_runtime': 44.2132, 'eval_samples_per_second': 113.088, 'eval_steps_per_second': 14.136, 'epoch': 0.0}
{'loss': 6.265, 'grad_norm': 0.89453125, 'learning_rate': 0.000398, 'epoch': 0.01}
{'eval_loss': 6.222482204437256, 'eval_runtime': 43.9462, 'eval_samples_per_second': 113.775, 'eval_steps_per_second': 14.222, 'epoch': 0.01}
{'loss': 5.2597, 'grad_norm': 0.609375, 'learning_rate': 0.00039998933331716213, 'epoch': 0.01}
{'eval_loss': 5.537166118621826, 'eval_runtime': 43.7743, 'eval_samples_per_second': 114.222, 'eval_steps_per_second': 14.278, 'epoch': 0.01}
{'loss': 4.7391, 'grad_norm': 0.5546875, 'learning_rate': 0.00039995690236805883, 'epoch': 0.01}
{'eval_loss': 5.181699275970459, 'eval_runtime': 43.5609, 'eval_samples_per_second': 114.782, 'eval_steps_per_second': 14.348, 'epoch': 0.01}
{'loss': 4.4188, 'grad_norm': 0.5703125, 'learning_rate': 0.00039990270959389834, 'epoch': 0.02}
{'eval_loss': 4.9442853927612305, 'eval_runtime': 44.4424, 'eval_samples_per_second': 112.505, 'eval_steps_per_second': 14.063, 'epoch': 0.02}
{'loss': 4.2083, 'grad_norm': 0.58203125, 'learning_rate': 0.0003998267608926199, 'epoch': 0.02}
{'eval_loss': 4.77987003326416, 'eval_runtime': 43.8136, 'eval_samples_per_second': 114.12, 'eval_steps_per_second': 14.265, 'epoch': 0.02}
{'loss': 4.0516, 'grad_norm': 0.50390625, 'learning_rate': 0.0003997290645299164, 'epoch': 0.02}
{'eval_loss': 4.606039524078369, 'eval_runtime': 43.7787, 'eval_samples_per_second': 114.211, 'eval_steps_per_second': 14.276, 'epoch': 0.02}
{'loss': 3.9021, 'grad_norm': 0.50390625, 'learning_rate': 0.0003996096311383347, 'epoch': 0.03}
{'eval_loss': 4.4763922691345215, 'eval_runtime': 43.5352, 'eval_samples_per_second': 114.849, 'eval_steps_per_second': 14.356, 'epoch': 0.03}
{'loss': 3.7558, 'grad_norm': 0.5, 'learning_rate': 0.0003994684737161189, 'epoch': 0.03}
{'eval_loss': 4.358518600463867, 'eval_runtime': 43.5265, 'eval_samples_per_second': 114.873, 'eval_steps_per_second': 14.359, 'epoch': 0.03}
Training stopped after 1800.63 seconds
{'train_runtime': 1801.3048, 'train_samples_per_second': 1077.032, 'train_steps_per_second': 16.829, 'train_loss': 4.944548399033214, 'epoch': 0.03}
Running final evaluation...
Final evaluation results: {'eval_loss': 4.358518600463867, 'eval_runtime': 43.6181, 'eval_samples_per_second': 114.631, 'eval_steps_per_second': 14.329, 'epoch': 0.03163606973790555}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

----------------------------------------------------------------------------------------------------

    Ð–Ð¸Ð»Ð¸-Ð±Ñ‹Ð»Ð¸ Ð´ÐµÐ´ Ð´Ð° Ð±Ð°Ð±Ð°.
    Ð‘Ñ‹Ð»Ð° Ñƒ Ð½Ð¸Ñ… ÐºÑƒÑ€Ð¾Ñ‡ÐºÐ° Ñ€ÑÐ±Ð°.
    Ð¡Ð½ÐµÑÐ»Ð° ÐºÑƒÑ€Ð¾Ñ‡ÐºÐ° ÑÐ¸Ñ‡ÐºÐ¾, Ð½Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ðµ - Ð·Ð¾Ð»Ð¾Ñ‚Ð¾Ðµ.

Ð’Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ð² Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» (1878) Ð¸ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» (1896).

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð² Ð´ÐµÑ€ÐµÐ²Ð½Ðµ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸÐ¾Ð¿Ð¾Ð²ÑÐºÐ¾Ð³Ð¾ ÑƒÐµÐ·Ð´Ð° Ñ‡Ð¸ÑÐ»Ð¸Ð»Ð¾ÑÑŒ Ð² 17 Ð²ÐµÐºÐµ.

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð² Ð´ÐµÑ€ÐµÐ²Ð½Ðµ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸÐ¾Ð¿Ð¾Ð²ÑÐºÐ¾Ð³Ð¾ ÑƒÐµÐ·Ð´Ð° Ñ‡Ð¸ÑÐ»Ð¸Ð»Ð¾ÑÑŒ Ð² 17 Ð²ÐµÐºÐµ.

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð´ÐµÑ€ÐµÐ²Ð½Ñ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸÐ¾Ð¿Ð¾Ð²ÑÐºÐ¾Ð³Ð¾ ÑƒÐµÐ·Ð´Ð° Ñ‡Ð¸ÑÐ»Ð¸Ð»Ð¾ÑÑŒ Ð² 17 Ð²ÐµÐºÐµ.

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð´ÐµÑ€ÐµÐ²Ð½Ñ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸÐ¾Ð¿Ð¾Ð²ÑÐºÐ¾Ð³Ð¾ ÑƒÐµÐ·Ð´Ð° Ñ‡Ð¸ÑÐ»Ð¸Ð»Ð¾ÑÑŒ Ð² 17 Ð²ÐµÐºÐµ.

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð´ÐµÑ€ÐµÐ²Ð½Ñ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸÐ¾Ð¿Ð¾Ð²ÑÐºÐ¾Ð³Ð¾ ÑƒÐµÐ·Ð´Ð° Ñ‡Ð¸ÑÐ»Ð¸Ð»Ð¾ÑÑŒ Ð² 17 Ð²ÐµÐºÐµ.

Ð’ Â«Ð¡Ð¿Ð¸ÑÐºÐµ Ð½Ð°ÑÐµÐ»Ñ‘Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¸Ð¼Ð¿ÐµÑ€Ð¸Ð¸Â» 1862 Ð³Ð¾Ð´Ð° Ð´ÐµÑ€ÐµÐ²Ð½Ñ Ð¡Ð¼Ð¾Ñ€Ð³Ð¾Ð½-ÐŸ
----------------------------------------------------------------------------------------------------
